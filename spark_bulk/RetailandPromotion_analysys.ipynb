{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this post we will explore Group By and Order By with retail dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.hadoop.fs._\n",
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Encoders\n",
    "import org.apache.spark.sql.functions.{expr,col,pow}\n",
    "import org.apache.spark.sql.types.StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@40bc738b\n",
       "sc = org.apache.spark.SparkContext@7f81dc3e\n",
       "spark = org.apache.spark.sql.SQLContext@25627c05\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@25627c05"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setAppName(\"Flight status\")\n",
    "val sc = new SparkContext(conf)\n",
    "val spark = new SQLContext(sc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read  Retail Transactions CSV file with infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reatil_t = [customer_id: string, trans_date: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: string, trans_date: string ... 1 more field]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reatil_t = spark.read.format(\"csv\").\n",
    "            option(\"header\",\"true\").\n",
    "option(\"delimiter\", \",\").\n",
    "option(\"inferSchema\", \"true\").\n",
    "csv(\"/user/viswatejaster9073/Retail_promotionResponse/Retail_Data_Transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+\n",
      "|customer_id|trans_date|tran_amount|\n",
      "+-----------+----------+-----------+\n",
      "|     CS5295| 11-Feb-13|         35|\n",
      "|     CS4768| 15-Mar-15|         39|\n",
      "+-----------+----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_t.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing the schema structure of our two data frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- trans_date: string (nullable = true)\n",
      " |-- tran_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_t.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below summary shows the total count of our columns, mean, standard deviation, Minimum value and Maximum values.\n",
    "### when you see the mean/standard deviation  for customer_id and trans_date is null which means they are non integer values(you can see schema strcuture at section 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+-----------------+\n",
      "|summary|customer_id|trans_date|      tran_amount|\n",
      "+-------+-----------+----------+-----------------+\n",
      "|  count|     125000|    125000|           125000|\n",
      "|   mean|       null|      null|        64.991912|\n",
      "| stddev|       null|      null|22.86000619473492|\n",
      "|    min|     CS1112| 01-Apr-12|               10|\n",
      "|    max|     CS9000| 31-Oct-14|              105|\n",
      "+-------+-----------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_t.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+\n",
      "|summary|customer_id|           response|\n",
      "+-------+-----------+-------------------+\n",
      "|  count|       6884|               6884|\n",
      "|   mean|       null|0.09398605461940732|\n",
      "| stddev|       null|0.29183051177768093|\n",
      "|    min|     CS1112|                  0|\n",
      "|    max|     CS9000|                  1|\n",
      "+-------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_p.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find the top 5 frequent customer and less frequent customers using DataFrames\n",
    "## In this example we are using groupBy and OrderBy to solve our business usecase\n",
    "### GroupBy - The GROUP BY statement is widely used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns. In our case we need to groupby the customers and counting them as shown below\n",
    "\n",
    "### OrderBy - We will use this whenever we want to sort our data etither in ascending or decsending order, In our case we need to find the top 5 frequent customers and top 5 least visiting customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this example we are using count operation in groupBy to check the count of customers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|     CS4424|   39|\n",
      "|     CS4320|   38|\n",
      "|     CS3799|   36|\n",
      "|     CS3805|   35|\n",
      "|     CS2620|   35|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_t.select(\"customer_id\").groupBy(\"customer_id\").count().orderBy($\"count\".desc).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|     CS8559|    4|\n",
      "|     CS7716|    4|\n",
      "|     CS7333|    4|\n",
      "|     CS8376|    4|\n",
      "|     CS7224|    4|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_t.select(\"customer_id\").groupBy(\"customer_id\").count().orderBy($\"count\".asc).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find the top 5 frequent customer and less frequent customers using <b>Spark SQL <b>\n",
    "## when we want to use Spark SQL we need to register our DataFrame as a table using createOrReplaceTempView(\"XXXXXXX\") as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reatil_t.createOrReplaceTempView(\"retail_t\") // We are exposing our retail_t dataframe as the table rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|     CS7224|    4|\n",
      "|     CS8559|    4|\n",
      "|     CS7716|    4|\n",
      "|     CS7333|    4|\n",
      "|     CS8376|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select customer_id,count(customer_id) as count from retail_t group by customer_id order by count limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|     CS4424|   39|\n",
      "|     CS4320|   38|\n",
      "|     CS3799|   36|\n",
      "|     CS2620|   35|\n",
      "|     CS3013|   35|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select customer_id,count(customer_id) as count from retail_t group by customer_id order by count desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the higest and least transaction dates using DafaFrames\n",
    "#### In this example we are using sum operation in groupBy to sum all the trascation amount on the perticular dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|trans_date|sum(tran_amount)|\n",
      "+----------+----------------+\n",
      "| 16-Jul-11|            8791|\n",
      "| 20-May-14|            8108|\n",
      "| 15-Nov-12|            8054|\n",
      "| 11-Aug-11|            7938|\n",
      "| 18-Sep-11|            7866|\n",
      "+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_t.groupBy(\"trans_date\").sum().orderBy($\"sum(tran_amount)\".desc).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|trans_date|sum(tran_amount)|\n",
      "+----------+----------------+\n",
      "| 17-Jan-15|            3648|\n",
      "| 18-Aug-12|            3792|\n",
      "| 26-Feb-14|            3830|\n",
      "| 25-Nov-11|            3887|\n",
      "| 02-Jul-13|            3915|\n",
      "+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_t.groupBy(\"trans_date\").sum().orderBy($\"sum(tran_amount)\".asc).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the higest and least transaction dates using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|trans_date|total|\n",
      "+----------+-----+\n",
      "| 17-Jan-15| 3648|\n",
      "| 18-Aug-12| 3792|\n",
      "| 26-Feb-14| 3830|\n",
      "| 25-Nov-11| 3887|\n",
      "| 02-Jul-13| 3915|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trans_date,sum(tran_amount) as total from retail_t group by trans_date order by total limit 5\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|trans_date|total|\n",
      "+----------+-----+\n",
      "| 16-Jul-11| 8791|\n",
      "| 20-May-14| 8108|\n",
      "| 15-Nov-12| 8054|\n",
      "| 11-Aug-11| 7938|\n",
      "| 18-Sep-11| 7866|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trans_date,sum(tran_amount) as total from retail_t group by trans_date order by total desc limit 5\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the minimum and maximum transaction dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|trans_date|max(tran_amount)|\n",
      "+----------+----------------+\n",
      "| 01-Feb-15|             105|\n",
      "| 20-Feb-12|             105|\n",
      "| 22-Jul-12|             105|\n",
      "| 17-Jan-15|             105|\n",
      "| 28-Aug-11|             105|\n",
      "+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_t.groupBy(\"trans_date\").max().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|trans_date|min(tran_amount)|\n",
      "+----------+----------------+\n",
      "| 01-Feb-15|              10|\n",
      "| 20-Feb-12|              10|\n",
      "| 22-Jul-12|              13|\n",
      "| 17-Jan-15|              31|\n",
      "| 28-Aug-11|              10|\n",
      "+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reatil_t.groupBy(\"trans_date\").min().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the minimum and maximum transaction dates using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "|trans_date|minimum_amt|max_amount|\n",
      "+----------+-----------+----------+\n",
      "| 01-Feb-15|         10|       105|\n",
      "| 20-Feb-12|         10|       105|\n",
      "| 22-Jul-12|         13|       105|\n",
      "| 17-Jan-15|         31|       105|\n",
      "| 28-Aug-11|         10|       105|\n",
      "+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trans_date,min(tran_amount) as minimum_amt,max(tran_amount) as max_amount from retail_t group by trans_date limit 5\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
